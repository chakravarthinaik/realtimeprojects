{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced CNN Architectures\n",
    "\n",
    "In this notebook, we will delve into some of the most influential and widely used advanced Convolutional Neural Network (CNN) architectures. These architectures have significantly pushed the boundaries of computer vision tasks, particularly in image classification and object recognition. We will explore their core ideas, architectural designs, and how they overcome limitations of simpler CNNs.\n",
    "\n",
    "For each architecture, we will provide a theoretical overview and demonstrate how to use pre-trained models in TensorFlow/Keras, allowing you to leverage their power without training from scratch.\n",
    "\n",
    "## 1. VGG (Visual Geometry Group)\n",
    "\n",
    "VGG is a classic CNN architecture known for its simplicity and depth. Developed by the Visual Geometry Group at the University of Oxford, VGG networks primarily focus on increasing the depth of the network by using very small (3x3) convolutional filters. This approach demonstrated that depth is a critical component for good performance in CNNs.\n",
    "\n",
    "### Key Characteristics of VGG:\n",
    "\n",
    "*   **Small Filters:** Uses only 3x3 convolutional filters throughout the network. Stacking multiple 3x3 convolutional layers can achieve a receptive field equivalent to larger filters (e.g., two 3x3 layers have a receptive field of 5x5, and three 3x3 layers have a receptive field of 7x7), while having fewer parameters and more non-linearities.\n",
    "*   **Increased Depth:** VGG networks are typically very deep, with VGG16 and VGG19 being the most common variants, having 16 and 19 layers respectively.\n",
    "*   **Max Pooling:** Employs 2x2 max pooling layers after some convolutional blocks to reduce spatial dimensions.\n",
    "*   **Fully Connected Layers:** Ends with three fully connected layers, similar to traditional neural networks, for classification.\n",
    "\n",
    "### Why VGG was important:\n",
    "\n",
    "VGG proved that increasing the depth of the network, with small convolutional filters, can lead to significant improvements in performance. Its uniform architecture made it easy to understand and implement, and it served as a strong baseline for many subsequent research efforts.\n",
    "\n",
    "### Using a Pre-trained VGG16 Model\n",
    "\n",
    "Let's demonstrate how to load and use a pre-trained VGG16 model from `tf.keras.applications`. This model has been trained on the ImageNet dataset, which contains millions of images across 1000 categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained VGG16 model with weights from ImageNet\n",
    "model = VGG16(weights=\'imagenet\')\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Prediction with VGG16\n",
    "\n",
    "To use the pre-trained VGG16 model, input images need to be preprocessed in a specific way (e.g., resized to 224x224 pixels, normalized). Let's load an example image and make a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess an image\n",
    "def load_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Create a batch dimension\n",
    "    img_array = preprocess_input(img_array) # Preprocess the image for VGG16\n",
    "    return img_array\n",
    "\n",
    "# Example usage (you'll need an image file, e.g., 'cat.jpg' in your working directory)\n",
    "# For demonstration, let's assume we have an image named 'dog.jpg'\n",
    "# You can download a sample image or use your own.\n",
    "# For instance, you can download one from: https://www.pexels.com/photo/adult-golden-retriever-1000529/\n",
    "\n",
    "# Create a dummy image for demonstration if no image is available\n",
    "try:\n",
    "    img_path = tf.keras.utils.get_file(\'yellow_lab.jpg\', \'https://storage.googleapis.com/download.tensorflow.org/example_images/yellow_lab_aspen.jpg\')\n",
    "except Exception as e:\n",
    "    print(f\"Could not download image: {e}. Please provide an image file (e.g., 'dog.jpg') in your working directory.\")\n",
    "    # Create a blank image if download fails\n",
    "    img_array = np.zeros((1, 224, 224, 3))\n",
    "else:\n",
    "    img_array = load_image(img_path)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Decode the predictions\n",
    "print(\'Predicted:\', decode_predictions(predictions, top=3)[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ResNet (Residual Network)\n",
    "\n",
    "ResNet, short for Residual Network, was introduced by Microsoft Research in 2015 and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) that year. The key innovation of ResNet is the introduction of \"residual blocks\" or \"skip connections\" that allow the network to train much deeper architectures without suffering from the vanishing gradient problem.\n",
    "\n",
    "### The Vanishing Gradient Problem:\n",
    "\n",
    "As neural networks get deeper, the gradients used for training can become extremely small (vanish) during backpropagation, making it difficult for the network to learn. This leads to performance degradation with increasing depth.\n",
    "\n",
    "### Residual Block:\n",
    "\n",
    "A residual block introduces a shortcut connection that bypasses one or more layers. The output of the block is the sum of the input to the block and the output of the stacked layers. Mathematically, if `H(x)` is the desired underlying mapping to be learned by a few stacked layers, the residual block learns `F(x) = H(x) - x`. The original mapping is then `H(x) = F(x) + x`. This makes it easier for the network to learn identity mappings, allowing deeper layers to simply pass through information if they don't need to learn anything new.\n",
    "\n",
    "### Key Characteristics of ResNet:\n",
    "\n",
    "*   **Residual Connections:** The defining feature, allowing gradients to flow directly through the network.\n",
    "*   **Deeper Networks:** Enabled the training of networks with hundreds or even thousands of layers (e.g., ResNet50, ResNet101, ResNet152).\n",
    "*   **Batch Normalization:** Widely used to stabilize training and accelerate convergence.\n",
    "\n",
    "### Using a Pre-trained ResNet50 Model\n",
    "\n",
    "Let's load and use a pre-trained ResNet50 model from `tf.keras.applications`. Like VGG, this model is also trained on the ImageNet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained ResNet50 model with weights from ImageNet\n",
    "model_resnet = ResNet50(weights=\'imagenet\")\n",
    "\n",
    "# Display the model summary\n",
    "model_resnet.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Prediction with ResNet50\n",
    "\n",
    "Similar to VGG, images need to be preprocessed for ResNet50. Let's use the same image loading function and make a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-using the load_image function from VGG section, ensuring target_size is (224, 224)\n",
    "# and using ResNet50's specific preprocess_input\n",
    "def load_image_resnet(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Create a batch dimension\n",
    "    img_array = preprocess_input(img_array) # Preprocess the image for ResNet50\n",
    "    return img_array\n",
    "\n",
    "# Example usage (you'll need an image file, e.g., 'cat.jpg' in your working directory)\n",
    "# For demonstration, let's assume we have an image named 'dog.jpg'\n",
    "# You can download a sample image or use your own.\n",
    "# For instance, you can download one from: https://www.pexels.com/photo/adult-golden-retriever-1000529/\n",
    "\n",
    "# Create a dummy image for demonstration if no image is available\n",
    "try:\n",
    "    img_path_resnet = tf.keras.utils.get_file(\'yellow_lab_resnet.jpg\', \'https://storage.googleapis.com/download.tensorflow.org/example_images/yellow_lab_aspen.jpg\')\n",
    "except Exception as e:\n",
    "    print(f\"Could not download image: {e}. Please provide an image file (e.g., 'dog.jpg') in your working directory.\")\n",
    "    # Create a blank image if download fails\n",
    "    img_array_resnet = np.zeros((1, 224, 224, 3))\n",
    "else:\n",
    "    img_array_resnet = load_image_resnet(img_path_resnet)\n",
    "\n",
    "# Make predictions\n",
    "predictions_resnet = model_resnet.predict(img_array_resnet)\n",
    "\n",
    "# Decode the predictions\n",
    "print(\'Predicted:\', decode_predictions(predictions_resnet, top=3)[0])\n"
   ]
  }
 ]
}


  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inception (GoogLeNet)\n",
    "\n",
    "The Inception architecture, also known as GoogLeNet, was introduced by Google in 2014 and was the winner of the ILSVRC 2014. Its main contribution is the \"Inception module,\" which allows the network to learn multiple feature representations at different scales simultaneously.\n",
    "\n",
    "### Inception Module:\n",
    "\n",
    "Instead of choosing one filter size (e.g., 3x3 or 5x5), an Inception module performs multiple convolutions with different filter sizes (1x1, 3x3, 5x5) and also a max pooling operation in parallel. The results of these operations are then concatenated along the channel dimension. This allows the network to capture information at various scales and combine them, leading to a richer representation.\n",
    "\n",
    "A key aspect of the Inception module is the use of 1x1 convolutions (also known as network in network or bottleneck layers). These 1x1 convolutions are used for two main purposes:\n",
    "\n",
    "1.  **Dimensionality Reduction:** They can reduce the number of channels before applying larger convolutions (e.g., 3x3 or 5x5), thereby reducing computational cost and preventing overfitting.\n",
    "2.  **Adding Non-linearity:** They introduce more non-linearity into the network without significantly increasing the computational burden.\n",
    "\n",
    "### Key Characteristics of Inception:\n",
    "\n",
    "*   **Parallel Convolutions:** Uses multiple filter sizes in parallel within the same module.\n",
    "*   **1x1 Convolutions:** Employed for dimensionality reduction and adding non-linearity.\n",
    "*   **Computational Efficiency:** Despite its complexity, Inception networks are computationally efficient due to the strategic use of 1x1 convolutions.\n",
    "*   **Deep and Wide:** The network is both deep (many layers) and wide (multiple operations in parallel).\n",
    "\n",
    "### Using a Pre-trained InceptionV3 Model\n",
    "\n",
    "InceptionV3 is a later version of the Inception architecture, incorporating further optimizations and improvements. Let\"s load and use a pre-trained InceptionV3 model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained InceptionV3 model with weights from ImageNet\n",
    "model_inception = InceptionV3(weights=\'imagenet\")\n",
    "\n",
    "# Display the model summary\n",
    "model_inception.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Prediction with InceptionV3\n",
    "\n",
    "InceptionV3 expects input images of size 299x299 pixels. We will adapt our image loading function accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_inception(img_path):\n",
    "    img = image.load_img(img_path, target_size=(299, 299))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Create a batch dimension\n",
    "    img_array = preprocess_input(img_array) # Preprocess the image for InceptionV3\n",
    "    return img_array\n",
    "\n",
    "# Example usage (you\"ll need an image file, e.g., \'cat.jpg\' in your working directory)\n",
    "# For demonstration, let\"s assume we have an image named \'dog.jpg\'\n",
    "# You can download a sample image or use your own.\n",
    "# For instance, you can download one from: https://www.pexels.com/photo/adult-golden-retriever-1000529/\n",
    "\n",
    "# Create a dummy image for demonstration if no image is available\n",
    "try:\n",
    "    img_path_inception = tf.keras.utils.get_file(\'yellow_lab_inception.jpg\', \'https://storage.googleapis.com/download.tensorflow.org/example_images/yellow_lab_aspen.jpg\')\n",
    "except Exception as e:\n",
    "    print(f\"Could not download image: {e}. Please provide an image file (e.g., \'dog.jpg\') in your working directory.\")\n",
    "    # Create a blank image if download fails\n",
    "    img_array_inception = np.zeros((1, 299, 299, 3))\n",
    "else:\n",
    "    img_array_inception = load_image_inception(img_path_inception)\n",
    "\n",
    "# Make predictions\n",
    "predictions_inception = model_inception.predict(img_array_inception)\n",
    "\n",
    "# Decode the predictions\n",
    "print(\'Predicted:\', decode_predictions(predictions_inception, top=3)[0])\n"
   ]
  }
 ]
}


  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DenseNet (Densely Connected Convolutional Network)\n",
    "\n",
    "DenseNet, introduced in 2017, builds upon the idea of skip connections from ResNet but takes it a step further. In a DenseNet, each layer is directly connected to every other layer in a feed-forward fashion. This means that the input to any layer includes the feature maps from all preceding layers, and its own feature maps are passed on to all subsequent layers.\n",
    "\n",
    "### Key Characteristics of DenseNet:\n",
    "\n",
    "*   **Dense Connectivity:** Each layer receives feature maps from all preceding layers and passes its own feature maps to all subsequent layers. This dense connectivity pattern encourages feature reuse and leads to more compact models.\n",
    "*   **Feature Reuse:** By concatenating feature maps from previous layers, DenseNet promotes feature reuse throughout the network, leading to a more efficient use of parameters.\n",
    "*   **Reduced Vanishing Gradient:** The direct connections to all preceding layers help alleviate the vanishing gradient problem, allowing for the training of very deep networks.\n",
    "*   **Fewer Parameters:** Despite the dense connections, DenseNets often have fewer parameters than other architectures with comparable accuracy because of the feature reuse and the use of bottleneck layers (1x1 convolutions) within dense blocks.\n",
    "*   **Transition Layers:** To manage the increasing number of feature maps and reduce spatial dimensions, DenseNets use \"transition layers\" between dense blocks. These layers typically consist of a 1x1 convolution followed by an average pooling layer.\n",
    "\n",
    "### Using a Pre-trained DenseNet121 Model\n",
    "\n",
    "Let\"s load and use a pre-trained DenseNet121 model from `tf.keras.applications`. This model is also trained on the ImageNet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained DenseNet121 model with weights from ImageNet\n",
    "model_densenet = DenseNet121(weights=\'imagenet\')\n",
    "\n",
    "# Display the model summary\n",
    "model_densenet.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Prediction with DenseNet121\n",
    "\n",
    "DenseNet121 expects input images of size 224x224 pixels. We will use a similar image loading function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_densenet(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Create a batch dimension\n",
    "    img_array = preprocess_input(img_array) # Preprocess the image for DenseNet121\n",
    "    return img_array\n",
    "\n",
    "# Example usage (you\"ll need an image file, e.g., \'cat.jpg\' in your working directory)\n",
    "# For demonstration, let\"s assume we have an image named \'dog.jpg\'\n",
    "# You can download a sample image or use your own.\n",
    "# For instance, you can download one from: https://www.pexels.com/photo/adult-golden-retriever-1000529/\n",
    "\n",
    "# Create a dummy image for demonstration if no image is available\n",
    "try:\n",
    "    img_path_densenet = tf.keras.utils.get_file(\'yellow_lab_densenet.jpg\', \'https://storage.googleapis.com/download.tensorflow.org/example_images/yellow_lab_aspen.jpg\')\n",
    "except Exception as e:\n",
    "    print(f\"Could not download image: {e}. Please provide an image file (e.g., \'dog.jpg\') in your working directory.\")\n",
    "    # Create a blank image if download fails\n",
    "    img_array_densenet = np.zeros((1, 224, 224, 3))\n",
    "else:\n",
    "    img_array_densenet = load_image_densenet(img_path_densenet)\n",
    "\n",
    "# Make predictions\n",
    "predictions_densenet = model_densenet.predict(img_array_densenet)\n",
    "\n",
    "# Decode the predictions\n",
    "print(\'Predicted:\', decode_predictions(predictions_densenet, top=3)[0])\n"
   ]
  }
 ]
}


  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. EfficientNet\n",
    "\n",
    "EfficientNet is a family of models developed by Google that achieves state-of-the-art accuracy with significantly fewer parameters and computational cost compared to previous CNNs. The core idea behind EfficientNet is \"compound scaling,\" which uniformly scales all dimensions of network depth, width, and resolution using a fixed set of scaling coefficients.\n",
    "\n",
    "### Compound Scaling:\n",
    "\n",
    "Traditional approaches to scaling CNNs often involve arbitrarily increasing one dimension (e.g., depth or width) or resolution. EfficientNet proposes a more principled approach by observing that there is a fixed relationship between these scaling dimensions. By scaling them together in a compound manner, EfficientNet achieves better performance and efficiency.\n",
    "\n",
    "The compound scaling method uses a compound coefficient \(\phi\) to uniformly scale network width, depth, and resolution:\n",
    "\n",
    "*   **Depth:** `d = α^φ`\n",
    "*   **Width:** `w = β^φ`\n",
    "*   **Resolution:** `r = γ^φ`\n",
    "\n",
    "where \(\alpha, \beta, \gamma\) are constants determined by a small grid search, and \(\phi\) is a user-specified coefficient that controls the overall scaling of the model.\n",
    "\n",
    "### Key Characteristics of EfficientNet:\n",
    "\n",
    "*   **Compound Scaling:** A novel method to uniformly scale network dimensions for optimal performance and efficiency.\n",
    "*   **Mobile Inverted Bottleneck Convolution (MBConv):** EfficientNet uses MBConv blocks as its main building blocks, which are also used in MobileNetV2. These blocks are designed for mobile and resource-constrained environments.\n",
    "*   **Squeeze-and-Excitation Networks:** Incorporates Squeeze-and-Excitation (SE) blocks, which adaptively recalibrate channel-wise feature responses, further improving performance.\n",
    "*   **Family of Models:** EfficientNet comes in a family of models (B0 to B7), each with different sizes and performance characteristics, allowing users to choose the right model for their specific needs.\n",
    "\n",
    "### Using a Pre-trained EfficientNetB0 Model\n",
    "\n",
    "Let\"s load and use a pre-trained EfficientNetB0 model, which is the baseline model in the EfficientNet family. This model is also trained on the ImageNet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained EfficientNetB0 model with weights from ImageNet\n",
    "model_efficientnet = EfficientNetB0(weights=\'imagenet\')\n",
    "\n",
    "# Display the model summary\n",
    "model_efficientnet.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Prediction with EfficientNetB0\n",
    "\n",
    "EfficientNetB0 expects input images of size 224x224 pixels. We will use a similar image loading function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_efficientnet(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Create a batch dimension\n",
    "    img_array = preprocess_input(img_array) # Preprocess the image for EfficientNetB0\n",
    "    return img_array\n",
    "\n",
    "# Example usage (you\"ll need an image file, e.g., \'cat.jpg\' in your working directory)\n",
    "# For demonstration, let\"s assume we have an image named \'dog.jpg\'\n",
    "# You can download a sample image or use your own.\n",
    "# For instance, you can download one from: https://www.pexels.com/photo/adult-golden-retriever-1000529/\n",
    "\n",
    "# Create a dummy image for demonstration if no image is available\n",
    "try:\n",
    "    img_path_efficientnet = tf.keras.utils.get_file(\'yellow_lab_efficientnet.jpg\', \'https://storage.googleapis.com/download.tensorflow.org/example_images/yellow_lab_aspen.jpg\')\n",
    "except Exception as e:\n",
    "    print(f\"Could not download image: {e}. Please provide an image file (e.g., \'dog.jpg\') in your working directory.\")\n",
    "    # Create a blank image if download fails\n",
    "    img_array_efficientnet = np.zeros((1, 224, 224, 3))\n",
    "else:\n",
    "    img_array_efficientnet = load_image_efficientnet(img_path_efficientnet)\n",
    "\n",
    "# Make predictions\n",
    "predictions_efficientnet = model_efficientnet.predict(img_array_efficientnet)\n",
    "\n",
    "# Decode the predictions\n",
    "print(\'Predicted:\', decode_predictions(predictions_efficientnet, top=3)[0])\n"
   ]
  }
 ]
}

